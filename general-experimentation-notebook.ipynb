{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# General Experimentation Notebook","metadata":{}},{"cell_type":"markdown","source":"## Set Experiment Parameters","metadata":{}},{"cell_type":"code","source":"# language parameters\nSEQ_LEN = 3\nVOCAB_SIZE = 9\n\n# dataset parameters\nIMG_DIM = 32\nMIN_SHAPES = 1\nMAX_SHAPES = 3\nOUTLINE = (255, 255, 255)\nALEC_MODE = True\n\n# training parameters\nBATCHES_TRAIN = 1\nBATCH_SIZE = 1024","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:45:18.848484Z","iopub.execute_input":"2022-04-16T21:45:18.849557Z","iopub.status.idle":"2022-04-16T21:45:18.854935Z","shell.execute_reply.started":"2022-04-16T21:45:18.849494Z","shell.execute_reply":"2022-04-16T21:45:18.853741Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset & Analysis Utils","metadata":{}},{"cell_type":"code","source":"!wget -O shapedata.py https://raw.githubusercontent.com/interactive-intelligence/emergent-lang/main/shapedata.py\nimport shapedata\nimport importlib\nimportlib.reload(shapedata)\n\n!wget -O analyzeutil.py https://raw.githubusercontent.com/interactive-intelligence/emergent-lang/main/analyzeutil.py\nimport analyzeutil\nimportlib.reload(analyzeutil)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:45:18.900361Z","iopub.execute_input":"2022-04-16T21:45:18.901021Z","iopub.status.idle":"2022-04-16T21:45:21.072242Z","shell.execute_reply.started":"2022-04-16T21:45:18.900979Z","shell.execute_reply":"2022-04-16T21:45:21.071289Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom PIL import Image\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\n# \"borrowed\" from https://github.com/zalandoresearch/pytorch-vq-vae\n# with slight adjustments to make it work for plain (non-2d) vectors\n\nclass VectorQuantizerEMA(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n        super(VectorQuantizerEMA, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.normal_()\n        self._commitment_cost = commitment_cost\n        \n        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n        self._ema_w.data.normal_()\n        \n        self._decay = decay\n        self._epsilon = epsilon\n\n    def forward(self, inputs):\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Use EMA to update the embedding vectors\n        if self.training:\n            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n                                     (1 - self._decay) * torch.sum(encodings, 0)\n            \n            # Laplace smoothing of the cluster size\n            n = torch.sum(self._ema_cluster_size.data)\n            self._ema_cluster_size = (\n                (self._ema_cluster_size + self._epsilon)\n                / (n + self._num_embeddings * self._epsilon) * n)\n            \n            dw = torch.matmul(encodings.t(), flat_input)\n            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n            \n            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n        \n        # Loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        loss = self._commitment_cost * e_latent_loss\n        \n        # Straight Through Estimator\n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        # return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n        return loss, quantized.contiguous(), perplexity, encodings\n\nclass VisionModule(nn.Module):\n    def __init__(self):\n        super(VisionModule, self).__init__()\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 16, 5),\n#             nn.LeakyReLU(),\n            nn.SiLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, 3),\n#             nn.LeakyReLU(),\n            nn.SiLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Flatten(),\n            \n            nn.Linear(32*6*6, 64),\n\n            nn.BatchNorm1d(64),\n        )\n\n    def forward(self, x):\n        return self.cnn(x)\n\n\nclass Model(nn.Module):\n    def __init__(self, no_vq=False):\n        super(Model, self).__init__()\n\n        self.no_vq = no_vq\n        \n        self.cnn = VisionModule()\n        self.encoderRNN = nn.GRU(64, 64, 1)\n        # self.encoderRNN = nn.LSTM(64, 64, 1)\n        self.vq = VectorQuantizerEMA(VOCAB_SIZE, 64, 0.25, 0.95)\n        self.decoderRNN = nn.GRU(64, 64, 1)\n        self.fc = nn.Sequential(\n            nn.Linear(128, 1),\n            nn.Sigmoid(),\n        )\n\n    def speak(self, z, max_len=SEQ_LEN):\n\n        seq = [ torch.zeros_like(z).unsqueeze(0) ]\n        h_n = z.unsqueeze(0)\n\n        for i in range(max_len):\n            # output, (h_n, c_n) = self.encoderRNN(seq[-1], (h_n, c_n))\n            output, h_n = self.encoderRNN(seq[-1], h_n)\n            seq.append(output)\n\n        return torch.cat(seq[1:], 0)\n    \n    def listen(self, seq, z):\n        output, h_n = self.decoderRNN(seq, z.unsqueeze(0))\n        fc_in = torch.cat([h_n.squeeze(0), z], axis=1)\n        return self.fc(fc_in)\n    \n    def forward(self, x):\n        z = self.cnn(x)\n        \n        seq = self.speak(z)\n\n        batch_size = x.shape[0]\n        assert batch_size % 2 == 0\n        left = slice(None, batch_size//2)\n        right = slice(batch_size//2, None)\n\n        if self.no_vq:\n            out1 = self.listen(seq[:, left], z[right])\n            out2 = self.listen(seq[:, right], z[left])\n            return torch.cat([out1, out2])\n        \n        loss, q_seq, _, enc = self.vq(seq)\n\n        out1 = self.listen(q_seq[:, left], z[right])\n        out2 = self.listen(q_seq[:, right], z[left])\n\n        seq_len = q_seq.shape[0]\n        enc = enc.reshape(seq_len, batch_size, -1)\n\n        return loss, torch.cat([out1, out2]), enc\n    \nbatch_size = BATCH_SIZE\ndevice = 'cpu'\n\nmodel = Model().to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\ndata = shapedata.ShapeData(batch_size=batch_size//2, im_size=IMG_DIM, min_shapes=MIN_SHAPES, max_shapes=MAX_SHAPES,\n                           alec_mode = ALEC_MODE, outline = OUTLINE)\n\nlosses = []\nbar = tqdm(range(BATCHES_TRAIN))\nfor batch in bar:\n\n    # get data from function\n    (x1, x1_shapes), (x2, x2_shapes), y = data.create_batch()\n    X, y = shapedata.to_pytorch_inputs(x1, x2, y)\n#     X = torch.from_numpy(X).to(torch.float32).to(device)\n#     y = torch.from_numpy(y).to(torch.float32).to(device)\n\n    # Compute prediction error\n    vq_loss, pred, enc = model(X)\n    pred = pred.flatten()\n\n    # vq_loss *= 0.1 # weird magic number I added for some reason\n    loss = criterion(pred, y) + vq_loss\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    bar.set_postfix({'loss': f'{loss.item():>6f}'})\n    losses.append(loss.item())\n    \nplt.figure(figsize=(10, 4), dpi=400)\nplt.plot(losses)\nplt.show()\nplt.close()","metadata":{"id":"B67uJ6ZsnEZs","execution":{"iopub.status.busy":"2022-04-16T21:45:21.076502Z","iopub.execute_input":"2022-04-16T21:45:21.077067Z","iopub.status.idle":"2022-04-16T21:45:21.883819Z","shell.execute_reply.started":"2022-04-16T21:45:21.077022Z","shell.execute_reply":"2022-04-16T21:45:21.882850Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"code","source":"def make_target_seq(text):\n    target_seq = ['_'] * SEQ_LEN\n    for i, c in enumerate(text):\n        if c.isnumeric():\n            target_seq[i] = int(c)\n        elif c == '_':\n            target_seq[i] = c\n        else:\n            break\n    return tuple(target_seq)\n\ndef draw_enc_images(text):\n    target_seq = make_target_seq(text)\n    print(target_seq)\n    \n    matches_target = lambda seq: analyzeutil.match_sequence(seq, target_seq)\n\n    df_sel = df[df['enc'].map(matches_target)]\n    if df_sel.empty:\n        return\n    \n    arr = analyzeutil.encoding_heatmap(df_sel['enc'], vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)\n    plt.imshow(arr, vmin=0)\n    plt.show()\n\n    return analyzeutil.sample_images(df_sel['images'])","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:45:21.885139Z","iopub.execute_input":"2022-04-16T21:45:21.885614Z","iopub.status.idle":"2022-04-16T21:45:21.893314Z","shell.execute_reply.started":"2022-04-16T21:45:21.885579Z","shell.execute_reply":"2022-04-16T21:45:21.892056Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"df = analyzeutil.create_dataset(model, data, n=25, progress=tqdm)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:45:21.895808Z","iopub.execute_input":"2022-04-16T21:45:21.896212Z","iopub.status.idle":"2022-04-16T21:45:28.955780Z","shell.execute_reply.started":"2022-04-16T21:45:21.896170Z","shell.execute_reply":"2022-04-16T21:45:28.954958Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"draw_enc_images('30_')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:46:06.244559Z","iopub.execute_input":"2022-04-16T21:46:06.245422Z","iopub.status.idle":"2022-04-16T21:46:06.272482Z","shell.execute_reply.started":"2022-04-16T21:46:06.245380Z","shell.execute_reply":"2022-04-16T21:46:06.270515Z"},"trusted":true},"execution_count":50,"outputs":[]}]}