{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Geometric Scene Sim Turing Test","metadata":{}},{"cell_type":"markdown","source":"## Set Game Parameters","metadata":{}},{"cell_type":"code","source":"'''\nLANGUAGE PARAMETERS\n'''\nSEQ_LEN = 3\nVOCAB_SIZE = 9\n\n'''\nDATASET PARAMETERS\n'''\nIMG_DIM = 32\nMIN_SHAPES = 1\nMAX_SHAPES = 3\nOUTLINE = (255, 255, 255)\nALEC_MODE = True\n\n'''\nMODEL TRAINING PARAMETERS\n'''\nBATCHES_TRAIN = 1\nBATCH_SIZE = 1024\n\n'''\nGAMEPLAY PARAMETERS\n'''\nSKIP_INTRO = False    # whether to skip introduction and instructions\nNUM_GAMES = 24        # number of rounds to play for\nANSWER_TIME = 3       # how much time the human has to answer a question\nBREAK_TIME = 2        # how much time between adjacent questions","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:55:25.854786Z","iopub.execute_input":"2022-04-16T21:55:25.855788Z","iopub.status.idle":"2022-04-16T21:55:25.885602Z","shell.execute_reply.started":"2022-04-16T21:55:25.855639Z","shell.execute_reply":"2022-04-16T21:55:25.884856Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset & Analysis Utils","metadata":{}},{"cell_type":"code","source":"!wget -O shapedata.py https://raw.githubusercontent.com/interactive-intelligence/emergent-lang/main/shapedata.py\nimport shapedata\nimport importlib\nimportlib.reload(shapedata)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:55:25.887467Z","iopub.execute_input":"2022-04-16T21:55:25.887907Z","iopub.status.idle":"2022-04-16T21:55:27.069355Z","shell.execute_reply.started":"2022-04-16T21:55:25.887875Z","shell.execute_reply":"2022-04-16T21:55:27.068399Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom PIL import Image\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\n# \"borrowed\" from https://github.com/zalandoresearch/pytorch-vq-vae\n# with slight adjustments to make it work for plain (non-2d) vectors\n\nclass VectorQuantizerEMA(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n        super(VectorQuantizerEMA, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.normal_()\n        self._commitment_cost = commitment_cost\n        \n        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n        self._ema_w.data.normal_()\n        \n        self._decay = decay\n        self._epsilon = epsilon\n\n    def forward(self, inputs):\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Use EMA to update the embedding vectors\n        if self.training:\n            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n                                     (1 - self._decay) * torch.sum(encodings, 0)\n            \n            # Laplace smoothing of the cluster size\n            n = torch.sum(self._ema_cluster_size.data)\n            self._ema_cluster_size = (\n                (self._ema_cluster_size + self._epsilon)\n                / (n + self._num_embeddings * self._epsilon) * n)\n            \n            dw = torch.matmul(encodings.t(), flat_input)\n            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n            \n            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n        \n        # Loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        loss = self._commitment_cost * e_latent_loss\n        \n        # Straight Through Estimator\n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        # return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n        return loss, quantized.contiguous(), perplexity, encodings\n\nclass VisionModule(nn.Module):\n    def __init__(self):\n        super(VisionModule, self).__init__()\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 16, 5),\n#             nn.LeakyReLU(),\n            nn.SiLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, 3),\n#             nn.LeakyReLU(),\n            nn.SiLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Flatten(),\n            \n            nn.Linear(32*6*6, 64),\n\n            nn.BatchNorm1d(64),\n        )\n\n    def forward(self, x):\n        return self.cnn(x)\n\n\nclass Model(nn.Module):\n    def __init__(self, no_vq=False):\n        super(Model, self).__init__()\n\n        self.no_vq = no_vq\n        \n        self.cnn = VisionModule()\n        self.encoderRNN = nn.GRU(64, 64, 1)\n        # self.encoderRNN = nn.LSTM(64, 64, 1)\n        self.vq = VectorQuantizerEMA(VOCAB_SIZE, 64, 0.25, 0.95)\n        self.decoderRNN = nn.GRU(64, 64, 1)\n        self.fc = nn.Sequential(\n            nn.Linear(128, 1),\n            nn.Sigmoid(),\n        )\n\n    def speak(self, z, max_len=SEQ_LEN):\n\n        seq = [ torch.zeros_like(z).unsqueeze(0) ]\n        h_n = z.unsqueeze(0)\n\n        for i in range(max_len):\n            # output, (h_n, c_n) = self.encoderRNN(seq[-1], (h_n, c_n))\n            output, h_n = self.encoderRNN(seq[-1], h_n)\n            seq.append(output)\n\n        return torch.cat(seq[1:], 0)\n    \n    def listen(self, seq, z):\n        output, h_n = self.decoderRNN(seq, z.unsqueeze(0))\n        fc_in = torch.cat([h_n.squeeze(0), z], axis=1)\n        return self.fc(fc_in)\n    \n    def forward(self, x):\n        z = self.cnn(x)\n        \n        seq = self.speak(z)\n\n        batch_size = x.shape[0]\n        assert batch_size % 2 == 0\n        left = slice(None, batch_size//2)\n        right = slice(batch_size//2, None)\n\n        if self.no_vq:\n            out1 = self.listen(seq[:, left], z[right])\n            out2 = self.listen(seq[:, right], z[left])\n            return torch.cat([out1, out2])\n        \n        loss, q_seq, _, enc = self.vq(seq)\n\n        out1 = self.listen(q_seq[:, left], z[right])\n        out2 = self.listen(q_seq[:, right], z[left])\n\n        seq_len = q_seq.shape[0]\n        enc = enc.reshape(seq_len, batch_size, -1)\n\n        return loss, torch.cat([out1, out2]), enc\n    \nbatch_size = BATCH_SIZE\ndevice = 'cpu'\n\nmodel = Model().to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\ndata = shapedata.ShapeData(batch_size=batch_size//2, im_size=IMG_DIM, min_shapes=MIN_SHAPES, max_shapes=MAX_SHAPES,\n                           alec_mode = ALEC_MODE, outline = OUTLINE)\n\nlosses = []\nbar = tqdm(range(BATCHES_TRAIN))\nfor batch in bar:\n\n    # get data from function\n    (x1, x1_shapes), (x2, x2_shapes), y = data.create_batch()\n    X, y = shapedata.to_pytorch_inputs(x1, x2, y)\n#     X = torch.from_numpy(X).to(torch.float32).to(device)\n#     y = torch.from_numpy(y).to(torch.float32).to(device)\n\n    # Compute prediction error\n    vq_loss, pred, enc = model(X)\n    pred = pred.flatten()\n\n    # vq_loss *= 0.1 # weird magic number I added for some reason\n    loss = criterion(pred, y) + vq_loss\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    bar.set_postfix({'loss': f'{loss.item():>6f}'})\n    losses.append(loss.item())\n    \nplt.figure(figsize=(10, 4), dpi=400)\nplt.plot(losses)\nplt.show()\nplt.close()","metadata":{"id":"B67uJ6ZsnEZs","execution":{"iopub.status.busy":"2022-04-16T21:55:27.071403Z","iopub.execute_input":"2022-04-16T21:55:27.072105Z","iopub.status.idle":"2022-04-16T21:55:29.922494Z","shell.execute_reply.started":"2022-04-16T21:55:27.072046Z","shell.execute_reply":"2022-04-16T21:55:29.921371Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Game\nPress `Shift` + `Enter` to execute the game. Jupyter Notebooks should auto-scroll for you, depending on the viewer.","metadata":{}},{"cell_type":"code","source":"from threading import Thread\nimport os\nimport time\n\n\n\"\"\"\nINTRODUCTION TEXT DYNAMICS\n\"\"\"\n\ndef clearConsole():\n    print('\\n'*15)\n\nwelcome_message = \"\"\"\n _ _ _     _                      _          _____                                 _____ __    _____ \n| | | |___| |___ ___ _____ ___   | |_ ___   |  |  |_ _ _____ ___ ___    _ _ ___   |   __|  |  |     |\n| | | | -_| |  _| . |     | -_|  |  _| . |  |     | | |     | .'|   |  | | |_ -|  |   __|  |__| | | |\n|_____|___|_|___|___|_|_|_|___|  |_| |___|  |__|__|___|_|_|_|__,|_|_|   \\_/|___|  |_____|_____|_|_|_|\n\"\"\"\n\nhorizontal_rule = '_____________________________________________________________________________________________________'\n\nprint(welcome_message)\ntime.sleep(1)\nprint(horizontal_rule)\ntime.sleep(1)\n\nif not SKIP_INTRO\n\n    instructions = \"\"\"\n    INSTRUCTIONS\n\n    You will be presented with two images of a scene. This scene will contain several objects. Each object\n    is either {red, green, blue} in color and either a {square, circle, triangle}. The objects may be in\n    different states of rotation and/or overlap.\n\n    Two geometric scenes are considered to be the same if they feature exactly the same set of objects, even if\n    the objects themselves are arranged in different ways.\n\n    A human will play against an Emergent Language Model. The human is given 3 seconds to enter either:\n    - 0 to indicate that they believe two scenes are different\n    - 1 to indicate that they believe two scenes are the same\n\n    After each guess, the answer will be displayed for two seconds. Then, the next question will be displayed.\n    In total, the game will run for 2 minutes nonstop, cycling through 2 * 60 / 5 = 24 continuous questions.\n\n    The human wins if they score a higher accuracy than the Emergent Language Model.\n    \"\"\"\n\n    for line in instructions.split('\\n'):\n        print(line)\n        time.sleep(0.5)\n    \n    print('\\nWhat is the name of the human challenging the Emergent Language Model?')\n    name = input(':: ')\n\n    print(f'\\nAre you ready to play, {name}? (Enter anything to continue)')\n    proceed = input(':: ')\n\ndef timer(seconds):\n    while seconds > 0:\n        print(f'SECONDS REMAINING: {seconds}...', end = \"\\r\")\n        seconds -= 1\n        time.sleep(1)\n    clearConsole()\n\n# prepare to play\nprint('\\nGet ready to play...')\ntimer(3)\nclearConsole()\n\n# track human vs model score\nscore = 0\nmodel_score = 0\n\n# create dataset and obtain model predictions ahead of time\n(x1, x1_shapes), (x2, x2_shapes), y = data.create_batch()\nX, y = shapedata.to_pytorch_inputs(x1, x2, y)\nloss, preds, enc = model.forward(X)\n\n# execute game play\nfor game in range(NUM_GAMES):\n    \n    print(f'GAME {game+1}/{NUM_GAMES}')\n    \n    # obtain model prediction\n    pred = preds[game]\n    \n    # show question\n    plt.subplot(1, 2, 1)\n    plt.imshow(x1[game])\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(x2[game])\n    plt.axis('off')\n    plt.show()\n    \n    # obtain user answer w/ time limit\n    answer = None\n    def check():\n        timer(ANSWER_TIME)\n        if answer != None:\n            return answer\n        print(\"You were too slow. Press enter to continue.\")\n    Thread(target = check).start()\n    answer = input(\"Answer (0/1): \")\n    \n    # border\n    print('-'*50 + '\\n')\n    \n    # calculate and print round results\n    print('END OF ROUND STATS:')\n    if len(answer) > 0 and answer[0] in ['0', '1'] and int(answer[0]) == int(y[game]):\n        print('Your answer was correct!')\n        score += 1\n    else:\n        print('Your answer was incorrect.')\n    print(f'Model predicted: {np.round(pred.mean().item(), 4)}')\n    if np.round(pred.mean().item()) == y[game]:\n        print(f'The model was correct!')\n        model_score += 1\n    else:\n        print('The model was incorrect!')\n    print(f'The correct answer was {y[game]}.')        \n    print(f'YOUR CURRENT ACCURACY:  {score / (game + 1)}')\n    print(f'MODEL CURRENT ACCURACY: {model_score / (game + 1)}')\n    print('\\n' + '-'*50 + '\\n')\n    \n    # next question\n    print('Next question in...')\n    timer(BREAK_TIME)\n    clearConsole()\n    \n# print final information\nprint('\\n' + '-'*50 + '\\n')\nprint('GAME FINISHED! FINAL STATS:')\nprint(f'YOUR ACCURACY:  {score / (game + 1)} ({score} / {NUM_GAMES})')\nprint(f'MODEL ACCURACY: {model_score / (game + 1)} ({model_score} / {NUM_GAMES})')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:55:29.924120Z","iopub.execute_input":"2022-04-16T21:55:29.924449Z","iopub.status.idle":"2022-04-16T21:55:32.057839Z","shell.execute_reply.started":"2022-04-16T21:55:29.924404Z","shell.execute_reply":"2022-04-16T21:55:32.056487Z"},"trusted":true},"execution_count":4,"outputs":[]}]}