{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DLSM Architecture Translation into TensorFlow - Attempt","metadata":{}},{"cell_type":"markdown","source":"## Utility Imports","metadata":{}},{"cell_type":"code","source":"!wget -O shapedata.py https://raw.githubusercontent.com/interactive-intelligence/emergent-lang/main/shapedata.py\nimport shapedata\nimport importlib\nimportlib.reload(shapedata)\n\n!wget -O analyzeutil.py https://raw.githubusercontent.com/interactive-intelligence/emergent-lang/main/analyzeutil.py\nimport analyzeutil\nimportlib.reload(analyzeutil)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-28T04:31:06.168588Z","iopub.execute_input":"2022-04-28T04:31:06.169041Z","iopub.status.idle":"2022-04-28T04:31:10.710197Z","shell.execute_reply.started":"2022-04-28T04:31:06.168919Z","shell.execute_reply":"2022-04-28T04:31:10.709350Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Library Imports","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import layers as L\nimport tensorflow_probability as tfp\nimport tensorflow as tf\nimport tensorflow\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-04-28T04:31:10.711848Z","iopub.execute_input":"2022-04-28T04:31:10.712076Z","iopub.status.idle":"2022-04-28T04:31:18.985556Z","shell.execute_reply.started":"2022-04-28T04:31:10.712049Z","shell.execute_reply":"2022-04-28T04:31:18.984878Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition","metadata":{}},{"cell_type":"code","source":"class VectorQuantizer(layers.Layer):\n    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.beta = (\n            beta  # This parameter is best kept between [0.25, 2] as per the paper.\n        )\n\n        # Initialize the embeddings which we will quantize.\n        w_init = tf.random_uniform_initializer()\n        self.embeddings = tf.Variable(\n            initial_value=w_init(\n                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n            ),\n            trainable=True,\n            name=\"embeddings_vqvae\",\n        )\n\n    def call(self, x):\n        # Calculate the input shape of the inputs and\n        # then flatten the inputs keeping `embedding_dim` intact.\n        input_shape = tf.shape(x)\n        flattened = tf.reshape(x, [-1, self.embedding_dim])\n\n        # Quantization.\n        encoding_indices = self.get_code_indices(flattened)\n        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n        quantized = tf.reshape(quantized, input_shape)\n\n        # Calculate vector quantization loss and add that to the layer. You can learn more\n        # about adding losses to different layers here:\n        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n        # the original paper to get a handle on the formulation of the loss function.\n        commitment_loss = self.beta * tf.reduce_mean(\n            (tf.stop_gradient(quantized) - x) ** 2\n        )\n        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n        self.add_loss(commitment_loss + codebook_loss)\n\n        # Straight-through estimator.\n        quantized = x + tf.stop_gradient(quantized - x)\n        return quantized\n\n    def get_code_indices(self, flattened_inputs):\n        # Calculate L2-normalized distance between the inputs and the codes.\n        similarity = tf.matmul(flattened_inputs, self.embeddings)\n        distances = (\n            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n            - 2 * similarity\n        )\n\n        # Derive the indices for minimum distances.\n        encoding_indices = tf.argmin(distances, axis=1)\n        return encoding_indices\n\n\n\nclass DLSM(tf.keras.Model):\n\n    def __init__(self, \n                 inp_shape,           # the dimension of the image in three-element tuple form\n                 seq_len=16,          # number of vectors to form a language sequence\n                 vocab_size=32,       # number of unique vectors in vector quantizer\n                 recurrent_units=32,  # number of gru units, also embedding dim for now\n                 batch_size=32):      # number of samples per batch\n        \n        super().__init__()\n        \n        # structural params\n        self.batch_size = batch_size\n        self.inp_shape = inp_shape\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        self.recurrent_units = recurrent_units\n        \n        # model components\n        self.vision_module = self.buildVisionModule(inp_shape)\n        self.speaker = self.buildSpeaker((self.seq_len, 1))\n        self.quantizer = VectorQuantizer(num_embeddings=self.vocab_size,\n                                         embedding_dim=self.recurrent_units)\n        self.listener = self.buildListener((self.seq_len, self.recurrent_units))\n        \n        # misc\n        self.initial_speech = K.variable(value=np.zeros((self.batch_size, self.seq_len, 1)))\n    \n    def buildVisionModule(self, inp_shape):\n        '''\n        Vision Module - maps images to a vector with length recurrent_units.\n        '''\n        \n        model = keras.models.Sequential(name='Vision_Module')\n        model.add(L.Input(inp_shape))\n\n        model.add(L.Conv2D(16, (5, 5), padding='same'))\n        model.add(L.LeakyReLU())\n        model.add(L.MaxPooling2D((2, 2)))\n\n        model.add(L.Conv2D(16, (3, 3), padding='same'))\n        model.add(L.LeakyReLU())\n        model.add(L.MaxPooling2D((2, 2)))\n\n        model.add(L.Flatten())\n        model.add(L.Dense(self.recurrent_units, activation='relu'))\n        model.add(L.BatchNormalization())\n        \n        return model\n        \n    def buildSpeaker(self, inp_shape):\n        '''\n        Speaker - takes in initial speech vector and uses image\n        vector as initial hidden state. Outputs a sequence of vectors\n        that are quantized.\n        '''\n        \n        init_speech = L.Input(inp_shape)\n        init_state = L.Input((self.recurrent_units,))\n        gru = L.GRU(self.recurrent_units, return_sequences=True)(init_speech, initial_state=init_state)\n        \n        return keras.models.Model(inputs={'init_speech':init_speech, 'init_state':init_state},\n                                  outputs=gru)\n\n    def buildListener(self, inp_shape):\n        '''\n        Listener - takes in the quantized 'language' and outputs a single\n        scalar probability.\n        '''\n        \n        init_speech = L.Input(inp_shape)\n        init_state = L.Input((self.recurrent_units,))\n        gru = L.GRU(self.recurrent_units)(init_speech, initial_state=init_state)\n        predense = L.Dense(32, activation='relu')(gru)\n        out = L.Dense(1, activation='sigmoid')(predense)\n        \n        \n        return keras.models.Model(inputs=[init_speech, init_state],\n                                  outputs=out)\n    \n    def call(self, inputs, training=True):\n        \n        # split data into half, Yegor-style\n        half = len(inputs) // 2\n        xa, xb = inputs[:half], inputs[half:]\n        \n        # get vision vectors\n        vision_a = self.vision_module(xa)\n        vision_b = self.vision_module(xb)\n        \n        # obtain spoken vectors {'init_speech':init_speech, 'init_state':init_state}\n        spoken_a = self.speaker({'init_speech':self.initial_speech, \n                                 'init_state':vision_a})\n        spoken_b = self.speaker({'init_speech':self.initial_speech, \n                                 'init_state':vision_b})\n        \n        # quantize speech\n        quantized_a = self.quantizer(spoken_a)\n        quantized_b = self.quantizer(spoken_b)\n        \n        # obtain output vectors after listening\n        listened_a = self.listener([quantized_a, vision_b])\n        listened_b = self.listener([quantized_b, vision_a])\n        \n        return listened_a, listened_b","metadata":{"execution":{"iopub.status.busy":"2022-04-28T04:36:09.841115Z","iopub.execute_input":"2022-04-28T04:36:09.841731Z","iopub.status.idle":"2022-04-28T04:36:09.874686Z","shell.execute_reply.started":"2022-04-28T04:36:09.841693Z","shell.execute_reply":"2022-04-28T04:36:09.873618Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import keras.backend as K\n\n'''\nCore data parameters\n'''\nBATCH_SIZE = 512\nSEQ_LEN = 4\nVOCAB_SIZE = 10\nRECURRENT_UNITS = 64\n\nIMG_DIM = 64\nMIN_SHAPES = 1\nMAX_SHAPES = 1\nSHAPE_TYPES = ['square', 'circle', 'triangle']\nCOLOR_TYPES = [(255,0,0), (0,255,  0), (0,0,255)]\nOUTLINE = (255, 255, 255)\nSHAPE_SCALE = 0.5\n\n# create dataset\ndata = shapedata.ShapeData(batch_size=BATCH_SIZE, \n                           im_size=IMG_DIM, \n                           min_shapes=MIN_SHAPES, \n                           max_shapes=MAX_SHAPES,\n                           outline = OUTLINE,\n                           shape_types = SHAPE_TYPES,\n                           shape_colors = COLOR_TYPES,\n                           shape_scale = SHAPE_SCALE)\n\n# create relevant training artifacts\noptimizer = tensorflow.keras.optimizers.Adam(learning_rate=1e-2)\nbce = tensorflow.keras.losses.BinaryCrossentropy()\nmodel = DLSM((IMG_DIM, IMG_DIM, 3), \n             seq_len=SEQ_LEN,\n             vocab_size=VOCAB_SIZE,\n             recurrent_units=RECURRENT_UNITS,\n             batch_size=BATCH_SIZE)\n\n# train batch function\n@tf.function\ndef train_batch(x, y):\n    with tf.GradientTape() as tape:\n        half = len(y) // 2\n        outa, outb = model(x, training=True)\n        loss = tf.math.divide(tf.math.add(bce(y, outa), bce(y, outb)), 2) # use avg bce as loss\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-28T04:43:21.593351Z","iopub.execute_input":"2022-04-28T04:43:21.594227Z","iopub.status.idle":"2022-04-28T04:43:22.099755Z","shell.execute_reply.started":"2022-04-28T04:43:21.594175Z","shell.execute_reply":"2022-04-28T04:43:22.098940Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"shapedata.demo_dataset(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T04:43:22.787036Z","iopub.execute_input":"2022-04-28T04:43:22.787478Z","iopub.status.idle":"2022-04-28T04:43:22.856150Z","shell.execute_reply.started":"2022-04-28T04:43:22.787445Z","shell.execute_reply":"2022-04-28T04:43:22.855271Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 1000\n\nlosses = []\nfor epoch in range(NUM_EPOCHS):\n    (x1, x1_shapes), (x2, x2_shapes), y = data.create_batch()\n    loss = train_batch(np.concatenate([x1, x2]), np.expand_dims(y,1)).numpy()\n    print(f'BATCH {epoch}: {loss}', end = '\\r')\n    losses.append(loss)\n    \nplt.figure(figsize=(10, 5), dpi=400)\nplt.plot(losses, color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T04:43:24.151864Z","iopub.execute_input":"2022-04-28T04:43:24.152364Z"},"trusted":true},"execution_count":null,"outputs":[]}]}